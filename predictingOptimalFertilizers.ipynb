{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91717,"databundleVersionId":12184666,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Final Project DTSA 5511 Introduction to Deep Learning - Predicting Optimal Fertilizers\n\nThis notebook provides a solution for the Predicting Optimal Fertilizers Kaggle competition, developed as a final project for the Deep Learning course at CU Boulder. It includes a problem explanation, exploratory data analysis (EDA), data preprocessing, model building with hyperparameter tuning, results analysis, and a conclusion.\n\n## 1. Deep Learning Problem Description\n\nThis project addresses a multiclass classification problem from the Kaggle Playground Series - Season 5, Episode 6. The objective is to predict the optimal 'Fertilizer Name' based on various agricultural features such as temperature, humidity, soil moisture, and nutrient levels (Nitrogen, Phosphorus, Potassium).\n\n**Business Context / Real-World Application:**\nAccurate prediction of the most suitable fertilizer type for specific crop and environmental conditions is crucial for optimizing agricultural yield, minimizing waste, reducing environmental impact from over-fertilization, and improving overall farm efficiency. This model can serve as a decision-support tool for farmers, enabling them to make data-driven choices about fertilizer application.\n\n**Dataset Source:**\nThe dataset is provided by the Kaggle Playground Series, a platform designed to help data scientists practice their skills on real-world-like problems.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport tensorflow as tf\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau","metadata":{"_uuid":"737e1d21-ac49-4a74-a150-1b590e820bf4","_cell_guid":"7b87012a-1f4e-47a5-990f-71f6f53d6ad2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. EDA","metadata":{}},{"cell_type":"code","source":"# Data Loading\ndata_dir = '/kaggle/input/playground-series-s5e6/'\ntry:\n    train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n    test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n    sample_submission_df = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))\n    print(\"Data loaded successfully from directory.\")\nexcept FileNotFoundError:\n    print(\"Error: Data files not found. Please ensure 'train.csv', 'test.csv', and 'sample_submission.csv' are in the directory.\")\n    exit()\n\nprint(\"\\n--- Train DataFrame Head: ---\")\nprint(train_df.head())\nprint(\"\\n--- Test DataFrame Head: ---\")\nprint(test_df.head())\n\n# Summaries\nprint(\"\\n--- Train DataFrame Descriptive Statistics: ---\")\nprint(train_df.describe())\nprint(\"\\n--- Test DataFrame Descriptive Statistics: ---\")\nprint(test_df.describe())\n\n# Missing Values\nprint(\"\\n--- Missing values in Train DataFrame: ---\")\nprint(train_df.isnull().sum())\nprint(\"\\n--- Missing values in Test DataFrame: ---\")\nprint(test_df.isnull().sum())\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Missing Value Handling Strategy:\nFor this dataset, it appears there are no missing values based on initial checks. If there were, a common strategy for numerical features might be mean/median imputation and for categorical features, mode imputation or a dedicated 'missing' category. Given no missing values, no specific handling is required at this stage.\n\n### Check Feature Distributions","metadata":{}},{"cell_type":"code","source":"\nnumerical_features = train_df.select_dtypes(include=np.number).columns.tolist()\n# Exclude 'ID' if present and target 'Fertilizer Name'\nif 'ID' in numerical_features:\n    numerical_features.remove('ID')\nif 'id' in numerical_features:  # Corrected column name based on output\n    numerical_features.remove('id')\n\n\nprint(\"\\n--- Distributions of Numerical Features: ---\")\nplt.figure(figsize=(15, 10))\nfor i, feature in enumerate(numerical_features):\n    plt.subplot(3, 4, i + 1)\n    sns.histplot(train_df[feature], kde=True)\n    plt.title(f'{feature} Distribution')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observation on Skewness:\nFeatures like 'Nitrogen', 'Phosphorus', 'Potassium' might exhibit some skewness. Log transformation or power transformation could be considered if they were highly skewed and a model sensitive to normality was used, but for deep learning, scaling is generally sufficient.\n","metadata":{}},{"cell_type":"code","source":"categorical_features = train_df.select_dtypes(include='object').columns.tolist()\nif 'Fertilizer Name' in categorical_features:\n    categorical_features.remove('Fertilizer Name') # Target variable\n\nprint(\"\\n--- Distributions of Categorical Features (if any - Target variable distribution shown below): ---\")\n\n# Target Variable Distribution\nplt.figure(figsize=(8, 6))\nsns.countplot(y=train_df['Fertilizer Name'], order=train_df['Fertilizer Name'].value_counts().index)\nplt.title('Distribution of Target Variable: Fertilizer Name')\nplt.show()\nprint(\"\\nTarget variable counts:\")\nprint(train_df['Fertilizer Name'].value_counts())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Observation:\nThe target variable 'Fertilizer Name' appears relatively balanced, which is good for classification.\n\n### Correlation Analysis","metadata":{}},{"cell_type":"code","source":"print(\"\\n--- Correlation Matrix of Numerical Features: ---\")\ncorrelation_matrix = train_df[numerical_features].corr()\nprint(correlation_matrix)\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Matrix of Numerical Features')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Justification for assumptions:\nHigh correlation between features (e.g., Temperature and Humidity if they were related) could indicate multicollinearity, which might affect traditional linear models but is less of a concern for deep learning models. Correlations with the target (if numerical) would indicate potential importance. Here, we can observe correlations among input features.\n\n### Outlier Detection:\nOutliers can be identified visually from box plots or distribution plots, or statistically using methods like IQR rule or Z-scores. For deep learning, scaling methods often mitigate the impact of outliers to some extent. I will rely on StandardScaler to handle the scale differences and implicitly reduce the relative impact of extreme values.\n\n### Data Transformation Hypothesis:\nBased on the numerical feature distributions, StandardScaler is hypothesized to be necessary for optimal deep learning model performance. Deep learning models, especially those with gradient-based optimization are sensitive to feature scales. Standardizing features (mean=0, variance=1) helps in faster convergence and prevents features with larger scale from dominating the learning process. No specific log transformation is initially planned as the chosen deep learning models are generally robust to non-normal distributions after scaling.\n\n### Feature Importance Intuition (based on research on Wikipedia):\nIntuitively, features like 'Nitrogen', 'Phosphorus', 'Potassium' (N, P, K values) are expected to be highly important as they directly relate to soil fertility and nutrient requirements.\n'Crop Type', 'Soil Type', 'Temparature', 'Humidity', and 'Moisture' are also likely to be very relevant as environmental and crop-specific factors directly influence fertilizer needs.\n(Domain knowledge from wikipedia)\n\n## 3. Model Building and Training (Deep Learning Focus)","metadata":{}},{"cell_type":"code","source":"# Separate features and target\nX = train_df.drop(['id', 'Fertilizer Name'], axis=1)\ny = train_df['Fertilizer Name']\nX_test = test_df.drop('id', axis=1)\n\n# Identify numerical and categorical features\nnumerical_cols = X.select_dtypes(include=np.number).columns.tolist()\ncategorical_cols = X.select_dtypes(include='object').columns.tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preprocessing Pipeline\nNumerical features will be scaled using StandardScaler. Categorical features will be One-Hot Encoded.","metadata":{}},{"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n    ])\n\n# Encode the target variable\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\nnum_classes = len(label_encoder.classes_)\nprint(f\"Number of target classes: {num_classes}\")\n\n# MAP@3 Evaluation Metric (Mean Average Precision at 3)\n# Custom metric function as required by the competition\ndef map_at_3(y_true, y_pred_proba):\n    \"\"\"\n    Calculates Mean Average Precision at 3 (MAP@3).\n    y_true: True labels (encoded integers).\n    y_pred_proba: Predicted probabilities for each class.\n    \"\"\"\n    # Get the top 3 predicted class indices for each sample\n    # sort in desc order of probability and take top 3 idxs\n    top_3_indices = np.argsort(y_pred_proba, axis=1)[:, ::-1][:, :3]\n\n    num_samples = y_true.shape[0]\n    avg_precision_sum = 0.0\n\n    for i in range(num_samples):\n        true_label = y_true[i]\n        predicted_labels_at_k = top_3_indices[i]\n\n        precision_at_k = 0.0\n        num_hits = 0\n        for k_idx, predicted_label in enumerate(predicted_labels_at_k):\n            if predicted_label == true_label:\n                num_hits += 1\n                # Precision at k: (Number of hits up to k) / k+1\n                precision_at_k += num_hits / (k_idx + 1)\n                break # Found the true label, no need to check further in top 3\n        if num_hits > 0: # Only add if the true label was found in the top 3\n            avg_precision_sum += precision_at_k\n\n    return avg_precision_sum / num_samples if num_samples > 0 else 0.0\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Selection and Definition\n\n### Model 1: Simple Multi-Layer Perceptron (MLP)","metadata":{}},{"cell_type":"code","source":"def create_mlp_model_1(input_shape, num_classes):\n    model = Sequential([\n        Dense(128, activation='relu', input_shape=(input_shape,)),\n        Dropout(0.3),\n        Dense(64, activation='relu'),\n        Dropout(0.3),\n        Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy']) # Accuracy for monitoring during training\n    return model\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model 2: Deeper Multi-Layer Perceptron (MLP)","metadata":{}},{"cell_type":"code","source":"def create_mlp_model_2(input_shape, num_classes):\n    model = Sequential([\n        Dense(256, activation='relu', input_shape=(input_shape,)),\n        Dropout(0.4),\n        Dense(128, activation='relu'),\n        Dropout(0.4),\n        Dense(64, activation='relu'),\n        Dropout(0.3),\n        Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Why these models:\nMLPs are a natural choice for tabular data as they can learn complex non-linear relationships between features. They are relatively straightforward to implement and interpret. Model 1 is a basic configuration to establish a baseline. Model 2 is slightly deeper and wider with more dropout to potentially capture more intricate patterns and prevent overfitting. The `softmax` activation is used in the output layer for multiclass classification and `sparse_categorical_crossentropy` is suitable as our target is integer-encoded.\n\n### Training Strategy: Stratified K-Fold Cross-Validation","metadata":{}},{"cell_type":"code","source":"N_SPLITS = 5\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\n# Callbacks for training\nearly_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=8, min_lr=0.00001, verbose=1)\n\nmodels_results = {}\nall_test_preds_proba_model1 = []\nall_test_preds_proba_model2 = []\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hyperparameter Optimization (Manual / Experimentation)\nFor this script, ***hyperparameter tuning*** is demonstrated through varying parameters between model 1 and model 2 (e.g., number of layers, units, dropout rates).\nA more exhaustive search could involve Keras Tuner or GridSearchCV with Keras wrappers. Search space for manual tuning (e.g., for `Dense` layers):\n- Number of layers: 2 to 4\n- Units per layer: 64, 128, 256\n- Dropout rates: 0.2, 0.3, 0.4\n- Optimizer: 'adam' (chosen for its good default performance)\n- Learning Rate: Managed by ReduceLROnPlateau\n- Batch size: 32 (a common good starting point)\n- Epochs: 100 (with EarlyStopping for efficiency)","metadata":{}},{"cell_type":"code","source":"print(f\"\\nTraining Models with {N_SPLITS}-Fold Stratified Cross-Validation...\")\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y_encoded)):\n    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} ---\")\n\n    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n    y_train_fold, y_val_fold = y_encoded[train_idx], y_encoded[val_idx]\n\n    # Apply preprocessing pipeline\n    X_train_processed = preprocessor.fit_transform(X_train_fold)\n    X_val_processed = preprocessor.transform(X_val_fold)\n    # Important: Transform test data using the preprocessor fitted on the *training data of the current fold*\n    X_test_processed = preprocessor.transform(X_test) \n\n    input_shape = X_train_processed.shape[1]\n\n    print(\"Training Model 1 (Simple MLP)...\")\n    model1 = create_mlp_model_1(input_shape, num_classes)\n    history1 = model1.fit(X_train_processed, y_train_fold,\n                          epochs=100,\n                          batch_size=32,\n                          validation_data=(X_val_processed, y_val_fold),\n                          callbacks=[early_stopping, reduce_lr],\n                          verbose=0)\n\n    val_loss_model1, val_acc_model1 = model1.evaluate(X_val_processed, y_val_fold, verbose=0)\n    print(f\"Model 1 - Fold {fold+1} Validation Accuracy: {val_acc_model1:.4f}\")\n\n    # Predict probabilities for validation set for MAP@3 calculation\n    y_val_pred_proba_model1 = model1.predict(X_val_processed)\n    fold_map3_model1 = map_at_3(y_val_fold, y_val_pred_proba_model1)\n    print(f\"Model 1 - Fold {fold+1} MAP@3: {fold_map3_model1:.4f}\")\n\n    # Predict probabilities for test set for ensemble/average prediction later\n    test_preds_proba_model1 = model1.predict(X_test_processed)\n    all_test_preds_proba_model1.append(test_preds_proba_model1)\n\n    # --- Model 2 Training ---\n    print(\"Training Model 2 (Deeper MLP)...\")\n    model2 = create_mlp_model_2(input_shape, num_classes)\n    history2 = model2.fit(X_train_processed, y_train_fold,\n                          epochs=100,\n                          batch_size=32,\n                          validation_data=(X_val_processed, y_val_fold),\n                          callbacks=[early_stopping, reduce_lr],\n                          verbose=0)\n\n    val_loss_model2, val_acc_model2 = model2.evaluate(X_val_processed, y_val_fold, verbose=0)\n    print(f\"Model 2 - Fold {fold+1} Validation Accuracy: {val_acc_model2:.4f}\")\n\n    y_val_pred_proba_model2 = model2.predict(X_val_processed)\n    fold_map3_model2 = map_at_3(y_val_fold, y_val_pred_proba_model2)\n    print(f\"Model 2 - Fold {fold+1} MAP@3: {fold_map3_model2:.4f}\")\n\n    test_preds_proba_model2 = model2.predict(X_test_processed)\n    all_test_preds_proba_model2.append(test_preds_proba_model2)\n\n    if 'Model 1 MAP@3' not in models_results:\n        models_results['Model 1 MAP@3'] = []\n        models_results['Model 2 MAP@3'] = []\n    models_results['Model 1 MAP@3'].append(fold_map3_model1)\n    models_results['Model 2 MAP@3'].append(fold_map3_model2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Results and Discussion/Conclusion","metadata":{}},{"cell_type":"code","source":"print(\"\\n--- Results and Discussion ---\")\n\n# Calculate average MAP@3\navg_map3_model1 = np.mean(models_results['Model 1 MAP@3'])\navg_map3_model2 = np.mean(models_results['Model 2 MAP@3'])\n\nprint(f\"\\nAverage MAP@3 for Model 1 (Simple MLP) across {N_SPLITS} folds: {avg_map3_model1:.4f}\")\nprint(f\"Average MAP@3 for Model 2 (Deeper MLP) across {N_SPLITS} folds: {avg_map3_model2:.4f}\")\n\n# Model Comparison\nprint(\"\\nModel Comparison:\")\nprint(f\"Model 1 MAP@3 scores per fold: {[f'{score:.4f}' for score in models_results['Model 1 MAP@3']]}\")\nprint(f\"Model 2 MAP@3 scores per fold: {[f'{score:.4f}' for score in models_results['Model 2 MAP@3']]}\")\n\nif avg_map3_model1 > avg_map3_model2:\n    print(\"\\nModel 1 (Simple MLP) performed better on average.\")\nelif avg_map3_model2 > avg_map3_model1:\n    print(\"\\nModel 2 (Deeper MLP) performed better on average.\")\nelse:\n    print(\"\\nBoth models performed similarly on average.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Strengths and Weaknesses:\n\n**Model 1 (Simple MLP):**\n- **Strengths:** Simpler, faster to train, less prone to overfitting on smaller datasets.\n- **Weaknesses:** May not capture very complex patterns as effectively as deeper models.\n\n**Model 2 (Deeper MLP):**\n- **Strengths:** Potentially capable of learning more complex representations, potentially higher accuracy on complex datasets.\n- **Weaknesses:** Longer training time, higher risk of overfitting if not properly regularized (e.g., with more dropout).\n\n### Error Analysis:\nTo perform a detailed error analysis, one would typically examine misclassified samples from the validation set. Key questions would include: Are certain fertilizer types consistently misclassified? Are there patterns in the features of misclassified samples (e.g., specific crop types, extreme environmental conditions)?\nWithout explicit misclassification examples here, we can infer that errors might arise from:\n- Ambiguous feature values where multiple fertilizers are equally plausible.\n- Boundary cases between different fertilizer types.\n- Insufficient representation of rare fertilizer combinations in the training data.\n\n### Limitations and Future Work:\n\n**Current Limitations:**\n- The models are relatively simple MLPs. While effective for tabular data, more advanced architectures could be explored.\n- Feature engineering was not extensively performed. More derived features (e.g., ratios, polynomial features) could be beneficial.\n- Hyperparameter tuning was manual/experiential. Automated tuning (e.g., Keras Tuner, Optuna) could find better configurations.\n- Ensemble methods (e.g., stacking predictions from different folds/models) were not explicitly used for the final prediction process, though averaging test predictions approximates an ensemble.\n- No external data was incorporated, which might provide more context for fertilizer recommendations.\n\n**Potential Improvements / Next Steps:**\n- **More Complex Architectures:** Explore Residual Networks for tabular data or attention mechanisms.\n- **Advanced Feature Engineering:** Create interaction terms, polynomial features or domain-specific features related to plant physiology or soil science.\n- **Ensemble Methods:** Implement sophisticated ensembling of multiple diverse models (e.g. XGBoost LightGBM and deep learning models with which I don't have experience yet but heard a lot of positive).\n- **Robust Outlier Handling:** Explicitly deal with outliers using robust scaling or outlier removal techniques if they significantly impact performance.\n- **Augment Data:** If possible, synthesize new data points, especially for under-represented classes.\n- **Explore Other Metrics:** While MAP@3 is primary, other metrcis could provide deeper insights.\n- **Interpretability:** Use further advanced techniques to understand feature importance for individual predictions.\n\n**Conclusion:**\nThis project successfully developed and evaluated Deep Learning models for predicting optimal fertilizer types, achieving competitive MAP@3 scores. The preprocessing pipeline effectively handled numerical scaling and categorical encoding. Stratified K-Fold cross-validation ensured robust evaluation. The results demonstrate the feasibility of using MLPs for this multiclass tabular classification problem. While the current models provide a strong baseline, there's significant scope for improvement through advanced feature engineering, more complex architectures, and comprehensive hyperparameter optimization. The insights gained from this project can contribute to more efficient and sustainable agricultural practices.\n\n## 5. Submission Generation","metadata":{}},{"cell_type":"code","source":"# vAverage probabilities from all folds for both models\n# This acts as a simple ensemble strategy\nensemble_test_preds_proba = (np.mean(all_test_preds_proba_model1, axis=0) +\n                             np.mean(all_test_preds_proba_model2, axis=0)) / 2\n\n# Get top 3 predicts for each sample\ntop_3_indices = np.argsort(ensemble_test_preds_proba, axis=1)[:, ::-1][:, :3]\n\n# Convert idxs back to original fertilizer names\npredicted_fertilizer_names = label_encoder.inverse_transform(top_3_indices.flatten()).reshape(-1, 3)\n\n# Format for submission\nsubmission_predictions = [' '.join(row) for row in predicted_fertilizer_names]\n\nsubmission_df = pd.DataFrame({'ID': test_df['id'], 'Predictions': submission_predictions}) # Corrected 'Id' to 'ID' for consistency\n\n# Save to CSV\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Submission file 'submission.csv' generated successfully!\")\nprint(\"\\nSample of generated submission file:\")\nprint(submission_df.head())\n\nprint(\"\\n--- Script Execution Complete ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}